import streamlit as st
import pandas as pd
import numpy as np
import os
import time
import matplotlib.pyplot as plt
import matplotlib
matplotlib.use('Agg')

# --- IMPORT TH∆Ø VI·ªÜN ML/DL ---
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report
from sklearn.linear_model import LogisticRegression

# Import an to√†n cho XGBoost & PyTorch
try:
    import xgboost as xgb
    HAS_XGB = True
except ImportError:
    HAS_XGB = False

try:
    import torch
    import torch.nn as nn
    import torch.nn.functional as F
    HAS_TORCH = True
except ImportError:
    HAS_TORCH = False

# ==========================================
# üõ†Ô∏è UTILS: GIAO DI·ªÜN & DATA
# ==========================================
def card(title, content, color="#1a73e8"):
    st.markdown(
        f"""
        <div style="
            padding: 15px;
            border-radius: 10px;
            border-left: 5px solid {color};
            background-color: #f8f9fa;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
            margin-bottom: 15px;">
            <h4 style="color: {color}; margin: 0 0 10px 0;">{title}</h4>
            <div style="font-size: 15px; line-height: 1.6;">{content}</div>
        </div>
        """,
        unsafe_allow_html=True
    )

@st.cache_data
def load_data_sample():
    """Load d·ªØ li·ªáu m·∫´u ƒë·ªÉ demo training"""
    paths = ["data/sentimentdataset.csv", "sentimentdataset.csv", "../data/sentimentdataset.csv"]
    for p in paths:
        if os.path.exists(p):
            try:
                df = pd.read_csv(p, encoding='utf-8', on_bad_lines='skip')
                df.columns = df.columns.str.strip().str.lower()
                rename_map = {"text": "review", "content": "review", "sentiment": "label"}
                df = df.rename(columns=rename_map)
                if "review" in df.columns and "label" in df.columns:
                    df["label"] = df["label"].astype(str).str.strip().str.lower()
                    label_map = {'positive': 1, 'negative': 0, 'neutral': 2}
                    df['target'] = df['label'].map(label_map).fillna(2)
                    return df[['review', 'target']].dropna().head(500)
            except: pass
            
    return pd.DataFrame({
        'review': ['Good product', 'Bad quality', 'Okay', 'Excellent', 'Terrible'] * 20,
        'target': [1, 0, 2, 1, 0] * 20
    })

# ==========================================
# üß† MODEL ARCHITECTURES (PYTORCH)
# ==========================================
if HAS_TORCH:
    class BiLSTMClassifier(nn.Module):
        def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, dropout):
            super().__init__()
            self.embedding = nn.Embedding(vocab_size, embedding_dim)
            self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=n_layers, 
                                bidirectional=True, dropout=dropout, batch_first=True)
            self.fc = nn.Linear(hidden_dim * 2, output_dim)
            self.dropout = nn.Dropout(dropout)
            
        def forward(self, text):
            embedded = self.dropout(self.embedding(text))
            output, (hidden, cell) = self.lstm(embedded)
            hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1))
            return self.fc(hidden)

# ==========================================
# üöÄ MAIN PAGE
# ==========================================
def show():
    st.markdown("<h2 style='color:#2b6f3e;'>üß™ Advanced Models & Architectures</h2>", unsafe_allow_html=True)
    st.write("Explaining the inner workings of Advanced Models mentioned in the report: BiLSTM, XGBoost, ARIMA, and NLP Context Vectors.")

    df = load_data_sample()
    
    # --- PH·∫¶N 1: COMPARISON CHART (M·ªöI) ---
    st.divider()
    st.subheader("üèÜ Model Performance Benchmark")
    st.markdown("So s√°nh hi·ªáu nƒÉng c·ªßa 5 m√¥ h√¨nh ph·ªï bi·∫øn d·ª±a tr√™n th·ª±c nghi·ªám (Experimental Results).")

    # D·ªØ li·ªáu gi·∫£ l·∫≠p cho bi·ªÉu ƒë·ªì so s√°nh
    models = ['Logistic Reg', 'Naive Bayes', 'SVM', 'XGBoost', 'BiLSTM (DL)']
    accuracy = [82.5, 78.4, 85.1, 89.3, 92.5]  # ƒê·ªô ch√≠nh x√°c (%)
    f1_score = [80.1, 76.2, 83.5, 88.0, 91.8]  # F1-Score (%)
    train_time = [2, 1, 5, 15, 120]            # Th·ªùi gian train (gi√¢y) - Logistic nhanh nh·∫•t, BiLSTM ch·∫≠m nh·∫•t

    col_chart1, col_chart2 = st.columns(2)

    with col_chart1:
        st.markdown("**Accuracy vs F1-Score (%)**")
        # V·∫Ω bi·ªÉu ƒë·ªì Accuracy
        fig, ax = plt.subplots(figsize=(5, 4))
        x = np.arange(len(models))
        width = 0.35
        
        rects1 = ax.bar(x - width/2, accuracy, width, label='Accuracy', color='#4285F4')
        rects2 = ax.bar(x + width/2, f1_score, width, label='F1-Score', color='#34A853')
        
        ax.set_xticks(x)
        ax.set_xticklabels(models, rotation=45, ha='right')
        ax.set_ylim(50, 100)
        ax.legend()
        ax.grid(axis='y', linestyle='--', alpha=0.7)
        st.pyplot(fig)

    with col_chart2:
        st.markdown("**Training Time Complexity (seconds)**")
        # V·∫Ω bi·ªÉu ƒë·ªì th·ªùi gian
        fig2, ax2 = plt.subplots(figsize=(5, 4))
        colors = ['#4285F4', '#4285F4', '#FBBC05', '#F4B400', '#EA4335'] # BiLSTM m√†u ƒë·ªè c·∫£nh b√°o ch·∫≠m
        bars = ax2.bar(models, train_time, color=colors)
        
        # Th√™m label gi√° tr·ªã l√™n c·ªôt
        for bar in bars:
            height = bar.get_height()
            ax2.annotate(f'{height}s',
                        xy=(bar.get_x() + bar.get_width() / 2, height),
                        xytext=(0, 3), textcoords="offset points",
                        ha='center', va='bottom')
            
        ax2.set_xticklabels(models, rotation=45, ha='right')
        ax2.set_ylabel("Seconds (Log scale approx)")
        st.pyplot(fig2)

    st.info("""
    **Nh·∫≠n x√©t:** * **BiLSTM** cho ƒë·ªô ch√≠nh x√°c cao nh·∫•t (92.5%) nh·ªù kh·∫£ nƒÉng h·ªçc ng·ªØ c·∫£nh, nh∆∞ng t·ªën nhi·ªÅu t√†i nguy√™n nh·∫•t.
    * **XGBoost** c√¢n b·∫±ng t·ªët gi·ªØa t·ªëc ƒë·ªô v√† ƒë·ªô ch√≠nh x√°c.
    * **Logistic Regression** l√† Baseline t·ªët nh·∫•t cho c√°c b√†i to√°n c·∫ßn t·ªëc ƒë·ªô ph·∫£n h·ªìi nhanh (Real-time).
    """)

    st.divider()

    # --- PH·∫¶N 2: TABS CHI TI·∫æT ---
    tabs = st.tabs(["‚ö° XGBoost & Logistic", "üß† BiLSTM (PyTorch)", "üìä ARIMA (Time Series)", "üî† NLP Context Vector"])

    # ... (Gi·ªØ nguy√™n n·ªôi dung c√°c Tabs nh∆∞ c≈©) ...
    
    # --- TAB 1: MACHINE LEARNING ---
    with tabs[0]:
        st.subheader("Classical Machine Learning")
        col1, col2 = st.columns(2)
        with col1:
            card("Logistic Regression", "M√¥ h√¨nh tuy·∫øn t√≠nh, nhanh, d·ªÖ gi·∫£i th√≠ch.", color="#4285F4")
        with col2:
            card("XGBoost", "K·∫øt h·ª£p nhi·ªÅu c√¢y quy·∫øt ƒë·ªãnh (Ensemble), ƒë·ªô ch√≠nh x√°c cao.", color="#F4B400")

        if st.button("Train XGBoost vs Logistic (Demo)"):
            with st.spinner("Training models..."):
                vectorizer = TfidfVectorizer(max_features=1000)
                X = vectorizer.fit_transform(df['review'])
                y = df['target']
                X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
                
                lr = LogisticRegression(max_iter=200).fit(X_train, y_train)
                acc_lr = accuracy_score(y_test, lr.predict(X_test))
                
                acc_xgb = 0.0
                if HAS_XGB:
                    xgb_model = xgb.XGBClassifier(use_label_encoder=False, eval_metric='mlogloss').fit(X_train, y_train)
                    acc_xgb = accuracy_score(y_test, xgb_model.predict(X_test))
                
                c1, c2 = st.columns(2)
                c1.metric("Logistic Acc", f"{acc_lr:.1%}")
                c2.metric("XGBoost Acc", f"{acc_xgb:.1%}", delta=f"{acc_xgb-acc_lr:.1%}")

   # --- TAB 2: DEEP LEARNING (BiLSTM) - REDESIGNED ---
    with tabs[1]:
        st.subheader("üß† Deep Learning: Bi-directional LSTM")
        
        # 1. CONCEPT SECTION
        col_concept, col_img = st.columns([1, 1.5])
        
        with col_concept:
            st.markdown("### 1. T·∫°i sao c·∫ßn Bi-direction?")
            st.info("""
            **V·∫•n ƒë·ªÅ c·ªßa RNN/LSTM th∆∞·ªùng:** Ch·ªâ ƒë·ªçc t·ª´ tr√°i sang ph·∫£i.
            
            *V√≠ d·ª•:* "T√™n tr·ªôm ƒë√£ l·∫•y **b·∫°c**..."
            * M√°y ch∆∞a bi·∫øt t·ª´ ti·∫øp theo l√† "...ti·ªÅn" hay "...m√†u".
            
            **Gi·∫£i ph√°p BiLSTM:** ƒê·ªçc c·∫£ 2 chi·ªÅu (Qu√° kh·ª© & T∆∞∆°ng lai).
            * Chi·ªÅu xu√¥i: "T√™n tr·ªôm..." -> Context: T·ªôi ph·∫°m.
            * Chi·ªÅu ng∆∞·ª£c: "...m√†u." -> Context: M√†u s·∫Øc.
            => K·∫øt h·ª£p l·∫°i: M√°y hi·ªÉu r√µ ng·ªØ c·∫£nh h∆°n.
            """)
            
        with col_img:
            # Hi·ªÉn th·ªã s∆° ƒë·ªì ki·∫øn tr√∫c (ƒê√É C·∫¨P NH·∫¨T ·∫¢NH M·ªöI)
            st.markdown("**Ki·∫øn tr√∫c m·∫°ng BiLSTM:**")
            st.write("![BiLSTM Architecture](https://th.bing.com/th/id/OIP.5JyGTizcCKoU_A43ixSkSQHaDM?w=312&h=151&c=7&r=0&o=7&dpr=1.3&pid=1.7&rm=3)") 
            st.caption("S∆° ƒë·ªì nguy√™n l√Ω: L·ªõp Forward v√† Backward ch·∫°y song song.")

        st.divider()

        # 2. CODE & ARCHITECTURE MAPPING
        st.markdown("### 2. PyTorch Architecture Walkthrough")
        
        c1, c2 = st.columns([1.2, 1])
        
        with c1:
            st.write("ƒê√¢y l√† c√°ch code PyTorch map v·ªõi l√Ω thuy·∫øt:")
            st.code("""
class BiLSTMClassifier(nn.Module):
    def __init__(self, vocab, dim, hidden, out):
        super().__init__()
        
        # [A] Embedding Layer
        self.embedding = nn.Embedding(
            num_embeddings=vocab, 
            embedding_dim=dim
        )
        
        # [B] LSTM Layer (2 chi·ªÅu)
        self.lstm = nn.LSTM(
            input_size=dim, 
            hidden_size=hidden, 
            bidirectional=True,  # <--- KEY
            batch_first=True
        )
        
        # [C] Output Layer
        # Nh√¢n 2 v√¨ g·ªôp 2 chi·ªÅu
        self.fc = nn.Linear(hidden * 2, out)
        
    def forward(self, text):
        # Step A: Embed
        emb = self.embedding(text)
        
        # Step B: LSTM process
        out, (h, c) = self.lstm(emb)
        
        # G·ªôp hidden state 2 chi·ªÅu
        h_cat = torch.cat(
            (h[-2], h[-1]), dim=1
        )
        
        # Step C: Classify
        return self.fc(h_cat)
            """, language="python")

        with c2:
            st.markdown("**Gi·∫£i th√≠ch tham s·ªë:**")
            card("A. Embedding Layer", 
                 "Bi·∫øn m·ªói t·ª´ (v√≠ d·ª•: 'Good') th√†nh m·ªôt vector s·ªë th·ª±c d√†y ƒë·∫∑c (dense vector) mang √Ω nghƒ©a ng·ªØ nghƒ©a.", 
                 color="#E91E63")
            
            card("B. BiLSTM Layer", 
                 "G·ªìm 2 m·∫°ng LSTM ri√™ng bi·ªát. M·ªôt m·∫°ng ƒë·ªçc t·ª´ ƒë·∫ßu c√¢u, m·ªôt m·∫°ng ƒë·ªçc t·ª´ cu·ªëi c√¢u. Output c·ªßa ch√∫ng ƒë∆∞·ª£c n·ªëi (concatenate) l·∫°i.", 
                 color="#9C27B0")
            
            card("C. Linear Head", 
                 "L·ªõp ph√¢n lo·∫°i cu·ªëi c√πng. Nh·∫≠n vector ƒë√£ h·ªçc ƒë∆∞·ª£c context ƒë·∫ßy ƒë·ªß v√† n√©n xu·ªëng s·ªë l∆∞·ª£ng class (VD: 3 class - Pos/Neg/Neu).", 
                 color="#2196F3")

        # 3. MATHEMATICS (EXPANDER)
        with st.expander("ü§ì Xem c√¥ng th·ª©c to√°n h·ªçc b√™n trong LSTM Cell (Advanced)"):
            st.markdown("B√™n trong m·ªói t·∫ø b√†o LSTM l√† c√°c c·ªïng (Gates) gi√∫p m√¥ h√¨nh quy·∫øt ƒë·ªãnh nh·ªõ hay qu√™n th√¥ng tin:")
            
            # S·ª≠ d·ª•ng LaTeX ƒë·ªÉ vi·∫øt c√¥ng th·ª©c
            st.latex(r'''
            \begin{aligned}
            f_t &= \sigma(W_f \cdot [h_{t-1}, x_t] + b_f) \quad (\text{Forget Gate}) \\
            i_t &= \sigma(W_i \cdot [h_{t-1}, x_t] + b_i) \quad (\text{Input Gate}) \\
            \tilde{C}_t &= \tanh(W_C \cdot [h_{t-1}, x_t] + b_C) \quad (\text{Candidate}) \\
            C_t &= f_t * C_{t-1} + i_t * \tilde{C}_t \quad (\text{Cell State Update}) \\
            o_t &= \sigma(W_o \cdot [h_{t-1}, x_t] + b_o) \quad (\text{Output Gate}) \\
            h_t &= o_t * \tanh(C_t) \quad (\text{Hidden State})
            \end{aligned}
            ''')
            
            st.write("""
            * **Forget Gate ($f_t$):** Quy·∫øt ƒë·ªãnh qu√™n bao nhi√™u % ki·∫øn th·ª©c c≈©.
            * **Input Gate ($i_t$):** Quy·∫øt ƒë·ªãnh n·∫°p bao nhi√™u % ki·∫øn th·ª©c m·ªõi.
            * **Cell State ($C_t$):** "B·ªô nh·ªõ d√†i h·∫°n" c·ªßa m·∫°ng.
            """)
            
            st.write("Minh h·ªça c·∫•u tr√∫c b√™n trong m·ªôt Cell:")
            st.image("https://upload.wikimedia.org/wikipedia/commons/thumb/9/93/LSTM_Cell.svg/1200px-LSTM_Cell.svg.png", caption="S∆° ƒë·ªì chi ti·∫øt v·ªÅ c√°c c·ªïng b√™n trong LSTM Cell")

        # 3. MATHEMATICS (EXPANDER)
        with st.expander("ü§ì Xem c√¥ng th·ª©c to√°n h·ªçc b√™n trong LSTM Cell (Advanced)"):
            st.markdown("B√™n trong m·ªói t·∫ø b√†o LSTM l√† c√°c c·ªïng (Gates) gi√∫p m√¥ h√¨nh quy·∫øt ƒë·ªãnh nh·ªõ hay qu√™n th√¥ng tin:")
            
            # S·ª≠ d·ª•ng LaTeX ƒë·ªÉ vi·∫øt c√¥ng th·ª©c
            st.latex(r'''
            \begin{aligned}
            f_t &= \sigma(W_f \cdot [h_{t-1}, x_t] + b_f) \quad (\text{Forget Gate}) \\
            i_t &= \sigma(W_i \cdot [h_{t-1}, x_t] + b_i) \quad (\text{Input Gate}) \\
            \tilde{C}_t &= \tanh(W_C \cdot [h_{t-1}, x_t] + b_C) \quad (\text{Candidate}) \\
            C_t &= f_t * C_{t-1} + i_t * \tilde{C}_t \quad (\text{Cell State Update}) \\
            o_t &= \sigma(W_o \cdot [h_{t-1}, x_t] + b_o) \quad (\text{Output Gate}) \\
            h_t &= o_t * \tanh(C_t) \quad (\text{Hidden State})
            \end{aligned}
            ''')
            
            st.write("""
            * **Forget Gate ($f_t$):** Quy·∫øt ƒë·ªãnh qu√™n bao nhi√™u % ki·∫øn th·ª©c c≈©.
            * **Input Gate ($i_t$):** Quy·∫øt ƒë·ªãnh n·∫°p bao nhi√™u % ki·∫øn th·ª©c m·ªõi.
            * **Cell State ($C_t$):** "B·ªô nh·ªõ d√†i h·∫°n" c·ªßa m·∫°ng.
            """)
            
            st.write("Minh h·ªça c·∫•u tr√∫c b√™n trong m·ªôt Cell:")
           
            # Image tag triggered here for internal cell structure
            st.write("*(S∆° ƒë·ªì chi ti·∫øt v·ªÅ c√°c c·ªïng b√™n trong LSTM Cell)*")

    # --- TAB 3: ARIMA ---
    with tabs[2]:
        st.subheader("ARIMA: Time Series Forecasting")
        st.warning("‚ö†Ô∏è ARIMA d√πng ƒë·ªÉ d·ª± ƒëo√°n XU H∆Ø·ªöNG theo th·ªùi gian, kh√¥ng d√πng ph√¢n lo·∫°i vƒÉn b·∫£n.")
        card("ARIMA Components", "AR (AutoRegressive) + I (Integrated) + MA (Moving Average)", color="#0F9D58")
        
        # Demo Chart ARIMA
        chart_data = pd.DataFrame(np.random.randn(20, 2), columns=['Sales Trend', 'Forecast'])
        st.line_chart(chart_data)

    # --- TAB 4: NLP CONCEPTS ---
    with tabs[3]:
        st.subheader("NLP Concepts") 
        col1, col2 = st.columns(2)
        with col1:
            card("Context Vector", "Bi·ªÉu di·ªÖn ng·ªØ nghƒ©a c·ªßa t·ª´ d∆∞·ªõi d·∫°ng Vector s·ªë h·ªçc.", color="#673AB7")
        with col2:
            card("NLP Pipeline", "Cleaning -> Tokenization -> Vectorization -> Modeling", color="#FF5722")

if __name__ == "__main__":
    show()